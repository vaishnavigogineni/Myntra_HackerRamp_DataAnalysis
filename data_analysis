import pandas as pd
import numpy as np

# Load the CSV file
file_path = '/content/couture_codex_myntraHackerRamp - Sheet1 (1).csv'
df = pd.read_csv(file_path)
print(df.head())
df['FOLLOWER COUNT'] = df['FOLLOWER COUNT'].replace({'K': '*1e3', 'M': '*1e6'}, regex=True).map(pd.eval).astype(int)
df['BRAND NAME'] = df['BRAND NAME'].str.strip()
df['PRODUCT CATEGORIES'] = df['PRODUCT CATEGORIES'].str.strip()
df['SOCIAL MEDIA HANDLES'] = df['SOCIAL MEDIA HANDLES'].str.strip()
df['UNIQUE SELLING POINTS(USPs)'] = df['UNIQUE SELLING POINTS(USPs)'].str.strip()
df['CONTACT INFORMATION'] = df['CONTACT INFORMATION'].str.strip()

print(df.head())


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Convert categorical columns to numerical using one-hot encoding
df_encoded = pd.get_dummies(df, columns=["PRODUCT CATEGORIES", "UNIQUE SELLING POINTS(USPs)"])
df_encoded = df_encoded.drop(columns=['BRAND NAME', 'SOCIAL MEDIA HANDLES', 'CONTACT INFORMATION'])
df_encoded = df_encoded.apply(pd.to_numeric, errors='coerce')
df_encoded = df_encoded.dropna()
X = df_encoded.drop(columns=['FOLLOWER COUNT'])
y = df_encoded['FOLLOWER COUNT']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")


import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
sns.histplot(df['FOLLOWER COUNT'], bins=30, kde=True)
plt.title('Distribution of Follower Counts')
plt.xlabel('Follower Count')
plt.ylabel('Frequency')
plt.show()
plt.figure(figsize=(14, 7))
sns.scatterplot(data=df, x='PRODUCT CATEGORIES', y='FOLLOWER COUNT', hue='BRAND NAME', s=100)
plt.title('Follower Count vs. Product Categories')
plt.xlabel('Product Categories')
plt.ylabel('Follower Count')
plt.xticks(rotation=45)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Convert categorical columns to numerical using one-hot encoding
df_encoded = pd.get_dummies(df, columns=["PRODUCT CATEGORIES", "UNIQUE SELLING POINTS(USPs)"])
df_encoded = df_encoded.drop(columns=['BRAND NAME', 'SOCIAL MEDIA HANDLES', 'CONTACT INFORMATION'])
df_encoded = df_encoded.apply(pd.to_numeric, errors='coerce')
df_encoded = df_encoded.dropna()
X = df_encoded.drop(columns=['FOLLOWER COUNT'])
y = df_encoded['FOLLOWER COUNT']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error, r2_score

# Model Training and Evaluation
models = {
    "Linear Regression": LinearRegression(),
    "Ridge Regression": Ridge(alpha=1.0),
    "Lasso Regression": Lasso(alpha=0.1)
}

for name, model in models.items():
    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')
    print(f"{name} CV Mean Squared Error: {-cv_scores.mean()}")
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"{name} Test Mean Squared Error: {mse}")
    print(f"{name} Test R2 Score: {r2}")


from sklearn.preprocessing import PolynomialFeatures

# Polynomial Regression
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X_train_scaled)

poly_model = LinearRegression()
poly_model.fit(X_poly, y_train)

X_test_poly = poly.transform(X_test_scaled)
y_poly_pred = poly_model.predict(X_test_poly)

mse_poly = mean_squared_error(y_test, y_poly_pred)
r2_poly = r2_score(y_test, y_poly_pred)
print(f"Polynomial Regression Test Mean Squared Error: {mse_poly}")
print(f"Polynomial Regression Test R2 Score: {r2_poly}")

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, label="Linear Regression", alpha=0.6)
plt.scatter(y_test, y_poly_pred, label="Polynomial Regression (Degree 2)", alpha=0.6)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=2)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs Predicted Follower Counts')
plt.legend()
plt.show()


import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

#the neural network model
model = Sequential()
model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='linear'))


model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mae'])
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
history = model.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test), epochs=100, batch_size=32, callbacks=[early_stopping])
loss, mae = model.evaluate(X_test_scaled, y_test)
print(f"Mean Absolute Error: {mae}")
y_pred = model.predict(X_test_scaled)

# Calculate Mean Squared Error and R2 Score
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"Neural Network Test Mean Squared Error: {mse}")
print(f"Neural Network Test R2 Score: {r2}")


plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, label="Neural Network Predictions", alpha=0.6)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=2)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs Predicted Follower Counts')
plt.legend()
plt.show()


from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Ridge, Lasso

ridge_params = {'alpha': [0.1, 1.0, 10.0]}
lasso_params = {'alpha': [0.01, 0.1, 1.0]}
ridge = Ridge()
lasso = Lasso()

# GridSearch for Ridge
ridge_grid = GridSearchCV(ridge, ridge_params, cv=5, scoring='neg_mean_squared_error')
ridge_grid.fit(X_train_scaled, y_train)
best_ridge = ridge_grid.best_estimator_  
print(f"Best Ridge parameters: {ridge_grid.best_params_}")

# GridSearch for Lasso
lasso_grid = GridSearchCV(lasso, lasso_params, cv=5, scoring='neg_mean_squared_error')
lasso_grid.fit(X_train_scaled, y_train)
best_lasso = lasso_grid.best_estimator_  
print(f"Best Lasso parameters: {lasso_grid.best_params_}")

for name, model in [("Ridge Regression", best_ridge), ("Lasso Regression", best_lasso)]:
    y_pred = model.predict(X_test_scaled)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"{name} Test Mean Squared Error: {mse}")
    print(f"{name} Test R2 Score: {r2}")

from sklearn.ensemble import VotingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
lr = LinearRegression()
ridge = Ridge(alpha=1.0)
lasso = Lasso(alpha=0.1)

ensemble = VotingRegressor(estimators=[('lr', lr), ('ridge', ridge), ('lasso', lasso)])
ensemble.fit(X_train_scaled, y_train)
y_pred_ensemble = ensemble.predict(X_test_scaled)
mse_ensemble = mean_squared_error(y_test, y_pred_ensemble)
r2_ensemble = r2_score(y_test, y_pred_ensemble)
print(f"Ensemble Test Mean Squared Error: {mse_ensemble}")
print(f"Ensemble Test R2 Score: {r2_ensemble}")


#LINK FOR THIS COLAB CODE: https://colab.research.google.com/drive/1VlzSkT6I2cnKWC6yo8_jL2GPN1CrtmKa?usp=sharing
